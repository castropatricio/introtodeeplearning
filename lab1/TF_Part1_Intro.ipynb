{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBk0ZDWY-ff8"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"https://i.ibb.co/Jr88sn2/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/MITDeepLearning/introtodeeplearning/blob/master/lab1/TF_Part1_Intro.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/MITDeepLearning/introtodeeplearning/blob/master/lab1/TF_Part1_Intro.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eI6DUic-6jo"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 MIT Introduction to Deep Learning. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of MIT Introduction\n",
        "# to Deep Learning must reference:\n",
        "#\n",
        "# © MIT Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4EzXzfGWGN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57knM8jrYZ2t"
      },
      "source": [
        "# Lab 1: Introducción a TensorFlow y la generación de música con RNN\n",
        "\n",
        "En este laboratorio, aprenderá a usar TensorFlow y cómo puede utilizarse para resolver tareas de aprendizaje profundo. Revise el código y ejecute cada celda. Durante el proceso, encontrará varios bloques ***TODO***; siga las instrucciones para completarlos antes de ejecutar esas celdas y continuar.\n",
        "\n",
        "\n",
        "# Part 1: Introducción a TensorFlow\n",
        "\n",
        "## 0.1 Instalar TensorFlow\n",
        "\n",
        "TensorFlow es una biblioteca de software ampliamente utilizada en aprendizaje automático. Aquí aprenderemos cómo se representan los cálculos y cómo definir una red neuronal simple en TensorFlow. En todos los laboratorios de TensorFlow de Introducción al Aprendizaje Profundo 2025, utilizaremos TensorFlow 2, que ofrece gran flexibilidad y la capacidad de ejecutar operaciones imperativamente, al igual que en Python. Observarás que TensorFlow 2 es bastante similar a Python en su sintaxis y ejecución imperativa. Instalemos TensorFlow y algunas dependencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkaimNJfYZ2w"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Descargar e importar el paquete Introducción al aprendizaje profundo del MIT\n",
        "!pip install mitdeeplearning --quiet\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QNMcdP4m3Vs"
      },
      "source": [
        "## 1.1 ¿Por qué TensorFlow se llama TensorFlow?\n",
        "\n",
        "Se denomina 'TensorFlow' porque gestiona el flujo (operación matemática/de nodos) de tensores, que son estructuras de datos que pueden considerarse como matrices multidimensionales. Los tensores se representan como matrices n-dimensionales de tipos de datos base, como cadenas o enteros, y permiten generalizar vectores y matrices a dimensiones superiores.\n",
        "\n",
        "La ```forma``` de un tensor define su número de dimensiones y el tamaño de cada una. El ```rango``` de un tensor proporciona el número de dimensiones (n-dimensiones); también se puede considerar como el orden o grado del tensor.\n",
        "\n",
        "Primero, veamos los tensores de dimensión cero, de los cuales un escalar es un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFxztZQInlAB"
      },
      "outputs": [],
      "source": [
        "sport = tf.constant(\"Tennis\", tf.string)\n",
        "number = tf.constant(1.41421356237, tf.float64)\n",
        "\n",
        "print(\"`sport` is a {}-d Tensor\".format(tf.rank(sport).numpy()))\n",
        "print(\"`number` is a {}-d Tensor\".format(tf.rank(number).numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dljcPUcoJZ6"
      },
      "source": [
        "Se pueden utilizar vectores y listas para crear tensores unidimensionales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaHXABe8oPcO"
      },
      "outputs": [],
      "source": [
        "sports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\n",
        "numbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)\n",
        "\n",
        "print(\"`sports` is a {}-d Tensor with shape: {}\".format(tf.rank(sports).numpy(), tf.shape(sports)))\n",
        "print(\"`numbers` is a {}-d Tensor with shape: {}\".format(tf.rank(numbers).numpy(), tf.shape(numbers)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvffwkvtodLP"
      },
      "source": [
        "\n",
        "A continuación, consideramos la creación de tensores bidimensionales (es decir, matrices) y de rango superior. Por ejemplo, en futuros laboratorios de procesamiento de imágenes y visión artificial, utilizaremos tensores cuatridimensionales. En este caso, las dimensiones corresponden al número de imágenes de ejemplo en nuestro lote, su altura, su ancho y el número de canales de color."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFeBBe1IouS3"
      },
      "outputs": [],
      "source": [
        "### Definición de tensores de orden superior ###\n",
        "\n",
        "'''TODO: Definir un tensor bidimensional'''\n",
        "matrix = # TODO\n",
        "\n",
        "assert isinstance(matrix, tf.Tensor), \"La matriz debe ser un objeto Tensor tf\"\n",
        "assert tf.rank(matrix).numpy() == 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv1fTn_Ya_cz"
      },
      "outputs": [],
      "source": [
        "'''TODO: Definir un tensor de 4 dimensiones.'''\n",
        "# Utilice tf.zeros para inicializar un tensor de ceros de 4 dimensiones con un tamaño de 10 x 256 x 256 x 3\n",
        "# Puedes pensar en esto como 10 imágenes donde cada imagen es RGB 256 x 256\n",
        "images = # TODO\n",
        "\n",
        "assert isinstance(images, tf.Tensor), \"La matriz debe ser un objeto Tensor tf\"\n",
        "assert tf.rank(images).numpy() == 4, \"La matriz debe ser de rango 4\"\n",
        "assert tf.shape(images).numpy().tolist() == [10, 256, 256, 3], \"La matriz tiene una forma incorrecta\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkaCDOGapMyl"
      },
      "source": [
        "Como has visto, la ```forma``` de un tensor indica el número de elementos en cada dimensión. La ```forma``` es muy útil y la usaremos con frecuencia. También puedes usar la segmentación para acceder a los subtensores dentro de un tensor de rango superior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhaufyObuLEG"
      },
      "outputs": [],
      "source": [
        "row_vector = matrix[1]\n",
        "column_vector = matrix[:,1]\n",
        "scalar = matrix[0, 1]\n",
        "\n",
        "print(\"`row_vector`: {}\".format(row_vector.numpy()))\n",
        "print(\"`column_vector`: {}\".format(column_vector.numpy()))\n",
        "print(\"`scalar`: {}\".format(scalar.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD3VO-LZYZ2z"
      },
      "source": [
        "## 1.2 Computations on Tensors\n",
        "\n",
        "Una forma práctica de visualizar los cálculos en TensorFlow es mediante grafos. Podemos definir este grafo en términos de tensores, que contienen datos, y las operaciones matemáticas que actúan sobre estos tensores en un orden determinado. Veamos un ejemplo sencillo y definamos este cálculo usando TensorFlow:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/MITDeepLearning/introtodeeplearning/2025/lab1/img/add-graph.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_YJrZsxYZ2z"
      },
      "outputs": [],
      "source": [
        "# Crea los nodos en el gráfico e inicializa los valores.\n",
        "a = tf.constant(15)\n",
        "b = tf.constant(61)\n",
        "\n",
        "# ¡Agregalos!\n",
        "c1 = tf.add(a,b)\n",
        "c2 = a + b # TensorFlow anula la operación \"+\" para poder actuar sobre los tensores\n",
        "print(c1)\n",
        "print(c2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbfv_QOiYZ23"
      },
      "source": [
        "Observe cómo hemos creado un grafo de cálculo compuesto por operaciones de TensorFlow y cómo la salida es un tensor con valor 76. Acabamos de crear un grafo de cálculo compuesto por operaciones, las hemos ejecutado y nos ha devuelto el resultado.\n",
        "\n",
        "Ahora consideremos un ejemplo un poco más complejo:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/MITDeepLearning/introtodeeplearning/2025/lab1/img/computation-graph.png)\n",
        "\n",
        "Aquí, tomamos dos entradas, `a, b`, y calculamos una salida `e`. Cada nodo del grafo representa una operación que toma una entrada, realiza un cálculo y pasa su salida a otro nodo.\n",
        "\n",
        "Definamos una función simple en TensorFlow para construir esta función de cálculo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJnfzpWyYZ23",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "### Definición de cálculos tensoriales ###\n",
        "\n",
        "# Construir una función de cálculo simple\n",
        "def func(a,b):\n",
        "  '''TODO: Define la operación para c, d, e (usa tf.add, tf.subtract, tf.multiply).'''\n",
        "  c = # TODO\n",
        "  d = # TODO\n",
        "  e = # TODO\n",
        "  return e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwrRfDMS2-oy"
      },
      "source": [
        "Ahora, podemos llamar a esta función para ejecutar el gráfico de cálculo dadas algunas entradas `a,b`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnwsf8w2uF7p"
      },
      "outputs": [],
      "source": [
        "# Considere valores de ejemplo para a,b\n",
        "a, b = 1.5, 2.5\n",
        "# Ejecutar el cálculo\n",
        "e_out = func(a,b)\n",
        "print(e_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HqgUIUhYZ29"
      },
      "source": [
        "Observe cómo nuestra salida es un tensor con valor definido por la salida del cálculo, y que la salida no tiene forma ya que es un único valor escalar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h4o9Bb0YZ29"
      },
      "source": [
        "## 1.3 Redes neuronales en TensorFlow\n",
        "\n",
        "También podemos definir redes neuronales en TensorFlow. TensorFlow utiliza una API de alto nivel llamada [Keras](https://www.tensorflow.org/guide/keras) que proporciona un framework potente e intuitivo para construir y entrenar modelos de aprendizaje profundo.\n",
        "\n",
        "Primero, consideremos el ejemplo de un perceptrón simple definido por una sola capa densa: $y = \\sigma(Wx + b)$, donde $W$ representa una matriz de pesos, $b$ es un sesgo, $x$ es la entrada, $\\sigma$ es la función de activación sigmoidea e $y$ es la salida. También podemos visualizar esta operación mediante un gráfico:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/MITDeepLearning/introtodeeplearning/2025/lab1/img/computation-graph-2.png)\n",
        "\n",
        "Los tensores pueden fluir a través de tipos abstractos llamados [```Capas```](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer), los componentes básicos de las redes neuronales. Las ```Capas``` implementan operaciones comunes de redes neuronales y se utilizan para actualizar ponderaciones, calcular pérdidas y definir la conectividad entre capas. Primero, definiremos una ```Capa``` para implementar el perceptrón simple definido anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HutbJk-1kHPh"
      },
      "outputs": [],
      "source": [
        "### Definición de una capa de red ###\n",
        "\n",
        "# n_output_nodes: número de nodos de salida\n",
        "# input_shape: forma de la entrada\n",
        "# x: entrada a la capa\n",
        "\n",
        "class OurDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(OurDenseLayer, self).__init__()\n",
        "    self.n_output_nodes = n_output_nodes\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    d = int(input_shape[-1])\n",
        "    # Definir e inicializar parámetros: una matriz de peso W y un sesgo b\n",
        "    # ¡Tenga en cuenta que la inicialización de parámetros es aleatoria!\n",
        "    self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes]) # tenga en cuenta la dimensionalidad\n",
        "    self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes]) # tenga en cuenta la dimensionalidad\n",
        "\n",
        "  def call(self, x):\n",
        "    '''TODO: define the operation for z (hint: use tf.matmul)'''\n",
        "    z = # TODO\n",
        "\n",
        "    '''TODO: define the operation for out (hint: use tf.sigmoid)'''\n",
        "    y = # TODO\n",
        "    return y\n",
        "\n",
        "# Dado que los parámetros de capa se inicializan aleatoriamente, estableceremos una semilla aleatoria para la reproducibilidad\n",
        "tf.keras.utils.set_random_seed(1)\n",
        "layer = OurDenseLayer(3)\n",
        "layer.build((1,2))\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "y = layer.call(x_input)\n",
        "\n",
        "# ¡Prueba la salida!\n",
        "print(y.numpy())\n",
        "mdl.lab1.test_custom_dense_layer_output(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt1FgM7qYZ3D"
      },
      "source": [
        "Convenientemente, TensorFlow ha definido una serie de ```Capas``` que se usan comúnmente en redes neuronales, por ejemplo, una [```Densa```](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable). Ahora, en lugar de usar una sola ```Capa``` para definir nuestra red neuronal simple, usaremos el modelo [`Secuencial`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Sequential) de Keras y una sola capa [`Densa`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) para definir nuestra red. Con la API `Secuencial`, puede crear fácilmente redes neuronales apilando capas como bloques de construcción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WXTpmoL6TDz"
      },
      "outputs": [],
      "source": [
        "### Definición de una red neuronal utilizando la API secuencial ###\n",
        "\n",
        "# Importar paquetes relevantes\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Definir el número de salidas\n",
        "n_output_nodes = 3\n",
        "\n",
        "# Primero defina el modelo\n",
        "model = Sequential()\n",
        "\n",
        "'''TODO: Definir una capa densa (completamente conectada) para calcular z'''\n",
        "# Recuerda: ¡las capas densas se definen mediante los parámetros W y b!\n",
        "# Puedes leer más sobre la inicialización de W y b en la documentación de TF :)\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable\n",
        "dense_layer = # TODO\n",
        "\n",
        "#Añadir la capa densa al modelo\n",
        "model.add(dense_layer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDGcwYfUyR-U"
      },
      "source": [
        "¡Listo! Hemos definido nuestro modelo usando la API Sequential. Ahora podemos probarlo con una entrada de ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg23OczByRDb"
      },
      "outputs": [],
      "source": [
        "# Modelo de prueba con entrada de ejemplo\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "'''TODO: ¡ingresar información al modelo y predecir la salida!'''\n",
        "model_output = # TODO\n",
        "print(model_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "596NvsOOtr9F"
      },
      "source": [
        "Además de definir modelos mediante la API `Sequential`, también podemos definir redes neuronales subclasificando directamente la clase [`Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable), que agrupa capas para facilitar el entrenamiento y la inferencia del modelo. La clase `Model` abarca lo que llamamos \"modelo\" o \"red\". Mediante la subclasificación, podemos crear una clase para nuestro modelo y luego definir el paso directo a través de la red mediante la función `call`. La subclasificación ofrece la flexibilidad de definir capas, bucles de entrenamiento, funciones de activación y modelos personalizados. Definamos la misma red neuronal que la anterior utilizando la subclasificación en lugar del modelo `Sequential`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4aCflPVyViD"
      },
      "outputs": [],
      "source": [
        "### Defining a model using subclassing ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class SubclassModel(tf.keras.Model):\n",
        "\n",
        "  # In __init__, we define the Model's layers\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(SubclassModel, self).__init__()\n",
        "    '''TODO: Our model consists of a single Dense layer. Define this layer.'''\n",
        "    self.dense_layer = '''TODO: Dense Layer'''\n",
        "\n",
        "  # In the call function, we define the Model's forward pass.\n",
        "  def call(self, inputs):\n",
        "    return self.dense_layer(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0-lwHDk4irB"
      },
      "source": [
        "Al igual que el modelo que construimos usando la API `Sequential`, probemos nuestro `SubclassModel` usando una entrada de ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhB34RA-4gXb"
      },
      "outputs": [],
      "source": [
        "n_output_nodes = 3\n",
        "model = SubclassModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "print(model.call(x_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTIFMJLAzsyE"
      },
      "source": [
        "Es importante destacar que la subclasificación nos brinda gran flexibilidad para definir modelos personalizados. Por ejemplo, podemos usar argumentos booleanos en la función de `llamada` para especificar diferentes comportamientos de la red, por ejemplo, durante el entrenamiento y la inferencia. Supongamos que, en algunos casos, queremos que nuestra red simplemente muestre la entrada, sin ninguna perturbación. Definimos un argumento booleano `isidentity` para controlar este comportamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7jzGX5D1xT5"
      },
      "outputs": [],
      "source": [
        "### Defining a model using subclassing and specifying custom behavior ###\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class IdentityModel(tf.keras.Model):\n",
        "\n",
        "  # As before, in __init__ we define the Model's layers\n",
        "  # Since our desired behavior involves the forward pass, this part is unchanged\n",
        "  def __init__(self, n_output_nodes):\n",
        "    super(IdentityModel, self).__init__()\n",
        "    self.dense_layer = tf.keras.layers.Dense(n_output_nodes, activation='sigmoid')\n",
        "\n",
        "  '''TODO: Implement the behavior where the network outputs the input, unchanged, under control of the isidentity argument.'''\n",
        "  def call(self, inputs, isidentity=False):\n",
        "    ### TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku4rcCGx5T3y"
      },
      "source": [
        "Probemos este comportamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzC0mgbk5dp2"
      },
      "outputs": [],
      "source": [
        "n_output_nodes = 3\n",
        "model = IdentityModel(n_output_nodes)\n",
        "\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "'''TODO: pass the input into the model and call with and without the input identity option.'''\n",
        "out_activate = # TODO\n",
        "out_identity = # TODO\n",
        "\n",
        "print(\"Network output with activation: {}; network identity output: {}\".format(out_activate.numpy(), out_identity.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V1dEqdk6VI5"
      },
      "source": [
        "Ahora que hemos aprendido cómo definir `Capas` y redes neuronales en TensorFlow usando las API `Secuenciales` y de Subclases, estamos listos para centrar nuestra atención en cómo implementar realmente el entrenamiento de red con retropropagación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQwDhKn8kbO2"
      },
      "source": [
        "## 1.4 Diferenciación automática en TensorFlow\n",
        "\n",
        "La [diferenciación automática](https://en.wikipedia.org/wiki/Automatic_differentiation) es uno de los componentes más importantes de TensorFlow y la base del entrenamiento con [retropropagación](https://en.wikipedia.org/wiki/Backpropagation). Utilizaremos la función GradientTape de TensorFlow [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape?version=stable) para rastrear las operaciones y calcular los gradientes posteriormente.\n",
        "\n",
        "Cuando se realiza un pase directo a través de la red, todas las operaciones de pase directo se graban en una \"cinta\"; posteriormente, para calcular el gradiente, la cinta se reproduce en sentido inverso. Por defecto, la cinta se descarta tras su reproducción en sentido inverso; esto significa que una función `tf.GradientTape` solo puede calcular un gradiente y las llamadas posteriores generan un error de ejecución. Sin embargo, podemos calcular múltiples gradientes en el mismo cálculo creando una cinta de gradientes ```persistente```.\n",
        "\n",
        "Primero, veremos cómo podemos calcular gradientes usando GradientTape y acceder a ellos para el cálculo. Definimos la función simple $y = x^2$ y calculamos el gradiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdkqk8pw5yJM"
      },
      "outputs": [],
      "source": [
        "### Gradient computation with GradientTape ###\n",
        "\n",
        "# y = x^2\n",
        "# Example: x = 3.0\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "# Initiate the gradient tape\n",
        "with tf.GradientTape() as tape:\n",
        "  # Define the function\n",
        "  y = x * x\n",
        "# Access the gradient -- derivative of y with respect to x\n",
        "dy_dx = tape.gradient(y, x)\n",
        "\n",
        "assert dy_dx.numpy() == 6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhU5metS5xF3"
      },
      "source": [
        "Al entrenar redes neuronales, utilizamos la diferenciación y el descenso de gradiente estocástico (SGD) para optimizar una función de pérdida. Ahora que comprendemos cómo se puede usar `GradientTape` para calcular y acceder a las derivadas, veremos un ejemplo donde usamos la diferenciación automática y el SGD para encontrar el mínimo de $L=(x-x_f)^2$. Aquí $x_f$ es una variable para un valor deseado que intentamos optimizar; $L$ representa una pérdida que intentamos minimizar. Si bien podemos resolver este problema analíticamente ($x_{min}=x_f$), considerar cómo podemos calcularlo usando `GradientTape` nos prepara para futuros laboratorios donde usemos el descenso de gradiente para optimizar las pérdidas de toda la red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "py"
          ],
          "id": ""
        },
        "id": "7g1yWiSXqEf-"
      },
      "outputs": [],
      "source": [
        "### Function minimization with automatic differentiation and SGD ###\n",
        "\n",
        "# Initialize a random value for our initial x\n",
        "x = tf.Variable([tf.random.normal([1])])\n",
        "print(\"Initializing x={}\".format(x.numpy()))\n",
        "\n",
        "learning_rate = 1e-2 # learning rate for SGD\n",
        "history = []\n",
        "# Define the target value\n",
        "x_f = 4\n",
        "\n",
        "# We will run SGD for a number of iterations. At each iteration, we compute the loss,\n",
        "#   compute the derivative of the loss with respect to x, and perform the SGD update.\n",
        "for i in range(500):\n",
        "  with tf.GradientTape() as tape:\n",
        "    '''TODO: define the loss as described above'''\n",
        "    loss = # TODO\n",
        "\n",
        "  # loss minimization using gradient tape\n",
        "  grad = tape.gradient(loss, x) # compute the derivative of the loss with respect to x\n",
        "  new_x = x - learning_rate*grad # sgd update\n",
        "  x.assign(new_x) # update the value of x\n",
        "  history.append(x.numpy()[0])\n",
        "\n",
        "# Plot the evolution of x as we optimize towards x_f!\n",
        "plt.plot(history)\n",
        "plt.plot([0, 500],[x_f,x_f])\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC7czCwk3ceH"
      },
      "source": [
        "`GradientTape` proporciona un marco extremadamente flexible para la diferenciación automática. Para propagar errores hacia atrás a través de una red neuronal, rastreamos los pases hacia adelante en la cinta, utilizamos esta información para determinar los gradientes y, a continuación, los utilizamos para la optimización mediante SGD."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WBk0ZDWY-ff8"
      ],
      "name": "TF_Part1_Intro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}